2025-07-24 12:31:33,207 [INFO] Saving results to data/tuning_results/jsons/tier1_lstm.json
2025-07-24 12:31:33,231 [INFO] Fold 52: Volatility=Medium, GA(pop=30, gen=15), BO(calls=10), top_n=5
2025-07-24 12:31:40,488 [WARNING] 5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x166fc3560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 12:31:40,542 [WARNING] 6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x166fc3560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 13:32:24,166 [INFO] Saving results to data/tuning_results/jsons/tier1_lstm.json
2025-07-24 13:32:24,187 [INFO] Fold 52: Volatility=Medium, GA(pop=30, gen=15), BO(calls=10), top_n=5
2025-07-24 13:32:30,066 [WARNING] 5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16c7d79c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 13:32:30,119 [WARNING] 6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16c7d79c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 13:34:39,538 [INFO] Saving results to data/tuning_results/jsons/tier1_lstm.json
2025-07-24 13:34:39,557 [INFO] Fold 52: Volatility=Medium, GA(pop=25, gen=15), BO(calls=10), top_n=5
2025-07-24 13:34:45,386 [WARNING] 5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x1582d7d80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 13:34:45,440 [WARNING] 6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x1582d7d80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 13:38:24,021 [INFO] Saving results to data/tuning_results/jsons/tier1_lstm.json
2025-07-24 13:38:24,033 [INFO] Starting fold 52
2025-07-24 13:38:24,040 [INFO] Fold 52: Volatility=Medium, GA(pop=25, gen=15), BO(calls=10), top_n=5
2025-07-24 13:38:28,900 [WARNING] 5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3021d7880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 13:38:28,954 [WARNING] 6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3021d7880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 13:49:38,451 [INFO] Saving results to data/tuning_results/jsons/tier1_lstm.json
2025-07-24 13:49:38,464 [INFO] Starting fold 52
2025-07-24 13:49:38,472 [INFO] Fold 52: Volatility=Medium, GA(pop=15, gen=10), BO(calls=7), top_n=4
2025-07-24 13:49:43,012 [WARNING] 5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x1619d7d80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 13:49:43,068 [WARNING] 6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x1619d7d80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 13:50:31,462 [INFO] Saving results to data/tuning_results/jsons/tier1_lstm.json
2025-07-24 13:50:31,473 [INFO] Starting fold 52
2025-07-24 13:50:31,481 [INFO] Fold 52: Volatility=Medium, GA(pop=15, gen=10), BO(calls=7), top_n=4
2025-07-24 13:50:35,694 [WARNING] 5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16c9d7d80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 13:50:35,750 [WARNING] 6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16c9d7d80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 13:56:01,557 [INFO] Fold 52: BO(n_calls=7)
2025-07-24 14:01:12,606 [INFO] Saving results to data/tuning_results/jsons/tier1_lstm.json
2025-07-24 14:01:12,618 [INFO] Starting fold 52
2025-07-24 14:01:12,625 [INFO] Fold 52: Volatility=Medium, GA(pop=15, gen=10), BO(calls=12), top_n=4
2025-07-24 14:01:12,634 [INFO] Evaluating: window=21, layers=2, units=55, dropout=0.339, lr=0.00164, batch=23
2025-07-24 14:01:14,335 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.383, lr=0.00030, batch=62
2025-07-24 14:01:15,985 [INFO] Evaluating: window=34, layers=1, units=37, dropout=0.173, lr=0.00311, batch=41
2025-07-24 14:01:16,859 [WARNING] 5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x15fdd7880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 14:01:16,912 [WARNING] 6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x15fdd7880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 14:01:17,095 [INFO] Evaluating: window=22, layers=1, units=51, dropout=0.156, lr=0.00299, batch=33
2025-07-24 14:01:18,205 [INFO] Evaluating: window=23, layers=2, units=38, dropout=0.306, lr=0.00596, batch=18
2025-07-24 14:01:20,114 [INFO] Evaluating: window=28, layers=1, units=34, dropout=0.480, lr=0.00966, batch=54
2025-07-24 14:01:21,216 [INFO] Evaluating: window=19, layers=1, units=53, dropout=0.276, lr=0.00131, batch=39
2025-07-24 14:01:22,333 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.365, lr=0.00319, batch=40
2025-07-24 14:01:23,889 [INFO] Evaluating: window=26, layers=1, units=63, dropout=0.410, lr=0.00940, batch=58
2025-07-24 14:01:25,081 [INFO] Evaluating: window=27, layers=2, units=34, dropout=0.178, lr=0.00055, batch=31
2025-07-24 14:01:26,984 [INFO] Evaluating: window=21, layers=1, units=58, dropout=0.243, lr=0.00288, batch=42
2025-07-24 14:01:28,199 [INFO] Evaluating: window=14, layers=2, units=34, dropout=0.495, lr=0.00775, batch=25
2025-07-24 14:01:30,085 [INFO] Evaluating: window=10, layers=2, units=54, dropout=0.392, lr=0.00774, batch=19
2025-07-24 14:01:32,077 [INFO] Evaluating: window=20, layers=1, units=59, dropout=0.349, lr=0.00338, batch=19
2025-07-24 14:01:33,401 [INFO] Evaluating: window=19, layers=1, units=55, dropout=0.355, lr=0.00888, batch=38
2025-07-24 14:01:34,653 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.383, lr=0.00337, batch=62
2025-07-24 14:01:36,489 [INFO] Evaluating: window=10, layers=2, units=54, dropout=0.392, lr=0.00777, batch=19
2025-07-24 14:01:38,458 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.178, lr=0.00055, batch=31
2025-07-24 14:01:40,426 [INFO] Evaluating: window=33, layers=1, units=37, dropout=0.162, lr=0.00028, batch=41
2025-07-24 14:01:41,759 [INFO] Evaluating: window=11, layers=1, units=55, dropout=0.355, lr=0.00856, batch=41
2025-07-24 14:01:43,030 [INFO] Evaluating: window=28, layers=1, units=34, dropout=0.488, lr=0.00966, batch=56
2025-07-24 14:01:44,315 [INFO] Evaluating: window=21, layers=2, units=55, dropout=0.339, lr=0.00741, batch=23
2025-07-24 14:01:46,195 [INFO] Evaluating: window=11, layers=2, units=34, dropout=0.495, lr=0.00315, batch=25
2025-07-24 14:01:48,198 [INFO] Evaluating: window=11, layers=2, units=50, dropout=0.365, lr=0.00051, batch=40
2025-07-24 14:01:49,992 [INFO] Evaluating: window=23, layers=1, units=34, dropout=0.480, lr=0.00962, batch=52
2025-07-24 14:01:51,357 [INFO] Evaluating: window=24, layers=2, units=55, dropout=0.294, lr=0.00164, batch=23
2025-07-24 14:01:53,554 [INFO] Evaluating: window=12, layers=2, units=53, dropout=0.404, lr=0.00314, batch=62
2025-07-24 14:01:55,532 [INFO] Evaluating: window=19, layers=2, units=40, dropout=0.365, lr=0.00319, batch=38
2025-07-24 14:01:57,598 [INFO] Evaluating: window=23, layers=2, units=38, dropout=0.267, lr=0.00596, batch=17
2025-07-24 14:01:59,837 [INFO] Evaluating: window=10, layers=2, units=54, dropout=0.391, lr=0.00197, batch=19
2025-07-24 14:02:02,048 [INFO] Evaluating: window=21, layers=2, units=55, dropout=0.316, lr=0.00734, batch=23
2025-07-24 14:02:04,107 [INFO] Evaluating: window=11, layers=2, units=34, dropout=0.366, lr=0.00319, batch=26
2025-07-24 14:02:06,211 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.188, lr=0.00030, batch=34
2025-07-24 14:02:08,306 [INFO] Evaluating: window=23, layers=2, units=36, dropout=0.299, lr=0.00165, batch=18
2025-07-24 14:02:10,583 [INFO] Evaluating: window=23, layers=2, units=38, dropout=0.365, lr=0.00315, batch=17
2025-07-24 14:02:12,894 [INFO] Evaluating: window=35, layers=2, units=38, dropout=0.275, lr=0.00311, batch=17
2025-07-24 14:02:15,377 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.391, lr=0.00030, batch=62
2025-07-24 14:02:17,570 [INFO] Evaluating: window=23, layers=2, units=38, dropout=0.267, lr=0.00604, batch=17
2025-07-24 14:02:19,986 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.493, lr=0.00315, batch=40
2025-07-24 14:02:22,016 [INFO] Evaluating: window=21, layers=2, units=33, dropout=0.329, lr=0.00055, batch=59
2025-07-24 14:02:24,246 [INFO] Evaluating: window=24, layers=2, units=53, dropout=0.283, lr=0.00596, batch=23
2025-07-24 14:02:26,677 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.306, lr=0.00600, batch=41
2025-07-24 14:02:28,905 [INFO] Evaluating: window=23, layers=1, units=37, dropout=0.165, lr=0.00546, batch=41
2025-07-24 14:02:30,596 [INFO] Evaluating: window=11, layers=2, units=32, dropout=0.469, lr=0.00311, batch=23
2025-07-24 14:02:32,930 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.308, lr=0.00599, batch=63
2025-07-24 14:02:35,265 [INFO] Evaluating: window=11, layers=2, units=52, dropout=0.391, lr=0.00308, batch=41
2025-07-24 14:02:37,600 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.178, lr=0.00055, batch=23
2025-07-24 14:02:39,953 [INFO] Evaluating: window=11, layers=1, units=40, dropout=0.477, lr=0.00315, batch=57
2025-07-24 14:02:41,747 [INFO] Evaluating: window=23, layers=2, units=40, dropout=0.476, lr=0.00596, batch=18
2025-07-24 14:02:44,321 [INFO] Evaluating: window=13, layers=2, units=51, dropout=0.183, lr=0.00055, batch=60
2025-07-24 14:02:46,659 [INFO] Evaluating: window=28, layers=1, units=34, dropout=0.383, lr=0.00966, batch=62
2025-07-24 14:02:48,425 [INFO] Evaluating: window=11, layers=2, units=34, dropout=0.361, lr=0.00318, batch=25
2025-07-24 14:02:50,854 [INFO] Evaluating: window=24, layers=1, units=55, dropout=0.302, lr=0.00164, batch=21
2025-07-24 14:02:52,809 [INFO] Evaluating: window=11, layers=2, units=39, dropout=0.365, lr=0.00030, batch=59
2025-07-24 14:02:55,206 [INFO] Evaluating: window=24, layers=2, units=55, dropout=0.294, lr=0.00164, batch=32
2025-07-24 14:02:57,788 [INFO] Evaluating: window=28, layers=2, units=33, dropout=0.488, lr=0.00966, batch=40
2025-07-24 14:03:00,313 [INFO] Evaluating: window=11, layers=2, units=38, dropout=0.341, lr=0.00315, batch=40
2025-07-24 14:03:02,695 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.386, lr=0.00030, batch=36
2025-07-24 14:03:05,278 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.476, lr=0.00030, batch=56
2025-07-24 14:03:07,768 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.497, lr=0.00315, batch=40
2025-07-24 14:03:10,110 [INFO] Evaluating: window=19, layers=2, units=32, dropout=0.386, lr=0.00030, batch=62
2025-07-24 14:03:12,627 [INFO] Evaluating: window=11, layers=2, units=35, dropout=0.178, lr=0.00055, batch=29
2025-07-24 14:03:16,028 [INFO] Evaluating: window=11, layers=2, units=34, dropout=0.388, lr=0.00044, batch=62
2025-07-24 14:03:20,008 [INFO] Evaluating: window=23, layers=2, units=37, dropout=0.476, lr=0.00961, batch=18
2025-07-24 14:03:23,617 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.391, lr=0.00030, batch=62
2025-07-24 14:03:26,283 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.412, lr=0.00030, batch=36
2025-07-24 14:03:29,466 [INFO] Evaluating: window=11, layers=2, units=44, dropout=0.402, lr=0.00030, batch=60
2025-07-24 14:03:32,162 [INFO] Evaluating: window=11, layers=2, units=55, dropout=0.391, lr=0.00030, batch=36
2025-07-24 14:03:34,836 [INFO] Evaluating: window=20, layers=2, units=40, dropout=0.493, lr=0.00392, batch=40
2025-07-24 14:03:37,560 [INFO] Evaluating: window=28, layers=1, units=50, dropout=0.483, lr=0.00956, batch=44
2025-07-24 14:03:39,777 [INFO] Evaluating: window=28, layers=2, units=38, dropout=0.317, lr=0.00598, batch=40
2025-07-24 14:03:42,575 [INFO] Evaluating: window=11, layers=2, units=39, dropout=0.353, lr=0.00030, batch=59
2025-07-24 14:03:45,262 [INFO] Evaluating: window=10, layers=2, units=51, dropout=0.444, lr=0.00030, batch=56
2025-07-24 14:03:47,948 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.383, lr=0.00030, batch=60
2025-07-24 14:03:50,475 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.365, lr=0.00030, batch=63
2025-07-24 14:03:53,237 [INFO] Evaluating: window=28, layers=1, units=34, dropout=0.489, lr=0.00040, batch=56
2025-07-24 14:03:55,470 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.363, lr=0.00030, batch=56
2025-07-24 14:03:58,713 [INFO] Evaluating: window=10, layers=2, units=38, dropout=0.387, lr=0.00356, batch=36
2025-07-24 14:04:01,544 [INFO] Evaluating: window=11, layers=2, units=42, dropout=0.391, lr=0.00030, batch=62
2025-07-24 14:04:04,750 [INFO] Evaluating: window=21, layers=2, units=35, dropout=0.178, lr=0.00055, batch=31
2025-07-24 14:04:08,188 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.391, lr=0.00030, batch=62
2025-07-24 14:04:11,128 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.386, lr=0.00030, batch=36
2025-07-24 14:04:14,092 [INFO] Evaluating: window=11, layers=2, units=38, dropout=0.306, lr=0.00030, batch=59
2025-07-24 14:04:16,974 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.382, lr=0.00960, batch=63
2025-07-24 14:04:19,813 [INFO] Evaluating: window=10, layers=2, units=40, dropout=0.448, lr=0.00319, batch=40
2025-07-24 14:04:22,800 [INFO] Evaluating: window=21, layers=2, units=33, dropout=0.493, lr=0.00030, batch=40
2025-07-24 14:04:25,779 [INFO] Evaluating: window=11, layers=2, units=50, dropout=0.365, lr=0.00030, batch=63
2025-07-24 14:04:28,738 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.493, lr=0.00294, batch=43
2025-07-24 14:04:31,765 [INFO] Evaluating: window=17, layers=2, units=41, dropout=0.378, lr=0.00030, batch=63
2025-07-24 14:04:34,739 [INFO] Evaluating: window=10, layers=2, units=51, dropout=0.383, lr=0.00030, batch=60
2025-07-24 14:04:37,753 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.360, lr=0.00030, batch=63
2025-07-24 14:04:40,876 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.491, lr=0.00309, batch=40
2025-07-24 14:04:43,885 [INFO] Evaluating: window=23, layers=2, units=36, dropout=0.309, lr=0.00017, batch=18
2025-07-24 14:04:47,345 [INFO] Evaluating: window=11, layers=2, units=39, dropout=0.406, lr=0.00030, batch=59
2025-07-24 14:04:50,381 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.167, lr=0.00031, batch=32
2025-07-24 14:04:53,705 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.492, lr=0.00294, batch=43
2025-07-24 14:04:56,833 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.496, lr=0.00330, batch=62
2025-07-24 14:04:59,924 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.476, lr=0.00030, batch=56
2025-07-24 14:05:03,134 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.183, lr=0.00069, batch=31
2025-07-24 14:05:06,343 [INFO] Evaluating: window=11, layers=2, units=50, dropout=0.369, lr=0.00037, batch=62
2025-07-24 14:05:09,468 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.383, lr=0.00586, batch=36
2025-07-24 14:05:12,711 [INFO] Evaluating: window=10, layers=2, units=51, dropout=0.365, lr=0.00030, batch=62
2025-07-24 14:05:15,897 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.386, lr=0.00030, batch=36
2025-07-24 14:05:19,272 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.367, lr=0.00030, batch=63
2025-07-24 14:05:23,224 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.388, lr=0.00029, batch=41
2025-07-24 14:05:26,838 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.365, lr=0.00030, batch=44
2025-07-24 14:05:30,134 [INFO] Evaluating: window=20, layers=2, units=52, dropout=0.481, lr=0.00030, batch=36
2025-07-24 14:05:33,688 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.178, lr=0.00055, batch=28
2025-07-24 14:05:37,339 [INFO] Evaluating: window=10, layers=2, units=51, dropout=0.383, lr=0.00027, batch=62
2025-07-24 14:05:40,862 [INFO] Evaluating: window=24, layers=2, units=34, dropout=0.386, lr=0.00030, batch=35
2025-07-24 14:05:44,347 [INFO] Evaluating: window=21, layers=2, units=40, dropout=0.476, lr=0.00010, batch=40
2025-07-24 14:05:47,734 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.491, lr=0.00309, batch=35
2025-07-24 14:05:51,128 [INFO] Evaluating: window=11, layers=2, units=37, dropout=0.383, lr=0.00299, batch=44
2025-07-24 14:05:54,540 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.492, lr=0.00266, batch=62
2025-07-24 14:05:58,306 [INFO] Evaluating: window=11, layers=2, units=33, dropout=0.385, lr=0.00330, batch=62
2025-07-24 14:06:01,717 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.383, lr=0.00030, batch=60
2025-07-24 14:06:04,935 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.366, lr=0.00030, batch=63
2025-07-24 14:06:08,299 [INFO] Evaluating: window=11, layers=2, units=38, dropout=0.493, lr=0.00321, batch=56
2025-07-24 14:06:11,663 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.391, lr=0.00030, batch=62
2025-07-24 14:06:15,008 [INFO] Evaluating: window=10, layers=2, units=51, dropout=0.492, lr=0.00024, batch=63
2025-07-24 14:06:18,389 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.383, lr=0.00030, batch=62
2025-07-24 14:06:21,844 [INFO] Evaluating: window=12, layers=2, units=41, dropout=0.492, lr=0.00295, batch=44
2025-07-24 14:06:25,273 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.476, lr=0.00030, batch=56
2025-07-24 14:06:28,792 [INFO] Evaluating: window=10, layers=2, units=51, dropout=0.492, lr=0.00024, batch=63
2025-07-24 14:06:32,225 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.365, lr=0.00030, batch=63
2025-07-24 14:06:35,691 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.386, lr=0.00041, batch=62
2025-07-24 14:06:39,194 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.472, lr=0.00030, batch=56
2025-07-24 14:06:42,767 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.492, lr=0.00227, batch=43
2025-07-24 14:06:46,292 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.386, lr=0.00030, batch=32
2025-07-24 14:06:49,900 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.493, lr=0.00314, batch=40
2025-07-24 14:06:53,355 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.366, lr=0.00050, batch=63
2025-07-24 14:06:56,908 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.383, lr=0.00030, batch=63
2025-07-24 14:07:00,586 [INFO] Evaluating: window=10, layers=2, units=43, dropout=0.492, lr=0.00032, batch=63
2025-07-24 14:07:04,320 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.391, lr=0.00030, batch=36
2025-07-24 14:07:08,533 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.369, lr=0.00030, batch=63
2025-07-24 14:07:12,299 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.489, lr=0.00294, batch=43
2025-07-24 14:07:16,387 [INFO] Evaluating: window=21, layers=2, units=37, dropout=0.391, lr=0.00013, batch=63
2025-07-24 14:07:20,390 [INFO] Evaluating: window=11, layers=2, units=34, dropout=0.387, lr=0.00029, batch=63
2025-07-24 14:07:24,157 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.493, lr=0.00315, batch=40
2025-07-24 14:07:27,705 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.493, lr=0.00315, batch=41
2025-07-24 14:07:31,537 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.387, lr=0.00030, batch=63
2025-07-24 14:07:35,255 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.492, lr=0.00294, batch=43
2025-07-24 14:07:39,155 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.418, lr=0.00030, batch=63
2025-07-24 14:07:42,947 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.365, lr=0.00030, batch=63
2025-07-24 14:07:46,815 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.326, lr=0.00030, batch=35
2025-07-24 14:07:50,720 [INFO] Evaluating: window=21, layers=2, units=41, dropout=0.365, lr=0.00030, batch=36
2025-07-24 14:07:54,822 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.493, lr=0.00314, batch=40
2025-07-24 14:07:58,700 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.496, lr=0.00330, batch=62
2025-07-24 14:08:02,823 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.365, lr=0.00030, batch=36
2025-07-24 14:08:07,385 [INFO] Evaluating: window=10, layers=2, units=51, dropout=0.492, lr=0.00024, batch=63
2025-07-24 14:08:11,542 [INFO] Fold 52: BO(n_calls=12)
2025-07-24 14:08:11,550 [INFO] Evaluating: window=34, layers=1, units=57, dropout=0.339, lr=0.00078, batch=21
2025-07-24 14:08:15,113 [INFO] Evaluating: window=24, layers=2, units=37, dropout=0.360, lr=0.00013, batch=51
2025-07-24 14:08:19,241 [INFO] Evaluating: window=38, layers=1, units=64, dropout=0.347, lr=0.00167, batch=16
2025-07-24 14:08:22,880 [INFO] Evaluating: window=11, layers=2, units=45, dropout=0.119, lr=0.00886, batch=27
2025-07-24 14:08:26,975 [INFO] Evaluating: window=13, layers=2, units=44, dropout=0.493, lr=0.00086, batch=57
2025-07-24 14:08:31,037 [INFO] Evaluating: window=30, layers=2, units=32, dropout=0.477, lr=0.00134, batch=34
2025-07-24 14:08:35,212 [INFO] Evaluating: window=10, layers=1, units=40, dropout=0.373, lr=0.00166, batch=56
2025-07-24 14:08:38,679 [INFO] Evaluating: window=15, layers=2, units=38, dropout=0.402, lr=0.00071, batch=26
2025-07-24 14:08:42,879 [INFO] Evaluating: window=27, layers=1, units=59, dropout=0.280, lr=0.00062, batch=60
2025-07-24 14:08:46,462 [INFO] Evaluating: window=32, layers=2, units=50, dropout=0.308, lr=0.00836, batch=57
2025-07-24 14:08:50,812 [INFO] Evaluating: window=34, layers=2, units=48, dropout=0.267, lr=0.01000, batch=16
2025-07-24 14:08:55,600 [INFO] Evaluating: window=10, layers=3, units=64, dropout=0.100, lr=0.00010, batch=16
2025-07-24 14:09:00,783 [INFO] â†’ Fold 52 done in 468.2s | RMSE=0.4361
2025-07-24 14:09:00,784 [INFO] Starting fold 60
2025-07-24 14:09:00,789 [INFO] Fold 60: Volatility=Medium, GA(pop=15, gen=10), BO(calls=12), top_n=4
2025-07-24 14:09:00,790 [INFO] Evaluating: window=14, layers=2, units=56, dropout=0.363, lr=0.00768, batch=56
2025-07-24 14:09:05,129 [INFO] Evaluating: window=28, layers=1, units=47, dropout=0.131, lr=0.00413, batch=35
2025-07-24 14:09:08,851 [INFO] Evaluating: window=11, layers=1, units=35, dropout=0.423, lr=0.00948, batch=19
2025-07-24 14:09:12,588 [INFO] Evaluating: window=38, layers=2, units=41, dropout=0.131, lr=0.00506, batch=54
2025-07-24 14:09:17,382 [INFO] Evaluating: window=31, layers=1, units=34, dropout=0.261, lr=0.00302, batch=27
2025-07-24 14:09:21,222 [INFO] Evaluating: window=18, layers=2, units=61, dropout=0.262, lr=0.00907, batch=31
2025-07-24 14:09:25,814 [INFO] Evaluating: window=24, layers=1, units=52, dropout=0.492, lr=0.00607, batch=33
2025-07-24 14:09:29,496 [INFO] Evaluating: window=29, layers=1, units=60, dropout=0.301, lr=0.00455, batch=44
2025-07-24 14:09:33,221 [INFO] Evaluating: window=28, layers=1, units=53, dropout=0.197, lr=0.00717, batch=55
2025-07-24 14:09:37,137 [INFO] Evaluating: window=34, layers=2, units=48, dropout=0.157, lr=0.00778, batch=29
2025-07-24 14:09:41,770 [INFO] Evaluating: window=24, layers=1, units=36, dropout=0.352, lr=0.00064, batch=51
2025-07-24 14:09:45,493 [INFO] Evaluating: window=19, layers=1, units=48, dropout=0.119, lr=0.00283, batch=49
2025-07-24 14:09:49,279 [INFO] Evaluating: window=11, layers=2, units=32, dropout=0.199, lr=0.00743, batch=31
2025-07-24 14:09:53,695 [INFO] Evaluating: window=13, layers=1, units=40, dropout=0.437, lr=0.00320, batch=53
2025-07-24 14:09:57,363 [INFO] Evaluating: window=36, layers=1, units=61, dropout=0.251, lr=0.00964, batch=20
2025-07-24 14:10:01,366 [INFO] Evaluating: window=10, layers=2, units=32, dropout=0.118, lr=0.00743, batch=31
2025-07-24 14:10:05,742 [INFO] Evaluating: window=18, layers=2, units=61, dropout=0.262, lr=0.00907, batch=31
2025-07-24 14:10:10,345 [INFO] Evaluating: window=28, layers=2, units=53, dropout=0.197, lr=0.00717, batch=55
2025-07-24 14:10:15,043 [INFO] Evaluating: window=37, layers=1, units=49, dropout=0.251, lr=0.00768, batch=18
2025-07-24 14:10:19,295 [INFO] Evaluating: window=28, layers=1, units=48, dropout=0.131, lr=0.00411, batch=29
2025-07-24 14:10:23,369 [INFO] Evaluating: window=12, layers=2, units=32, dropout=0.199, lr=0.00710, batch=29
2025-07-24 14:10:27,900 [INFO] Evaluating: window=36, layers=1, units=48, dropout=0.117, lr=0.00965, batch=20
2025-07-24 14:10:31,903 [INFO] Evaluating: window=19, layers=2, units=56, dropout=0.363, lr=0.00768, batch=56
2025-07-24 14:10:36,455 [INFO] Evaluating: window=19, layers=1, units=48, dropout=0.200, lr=0.00283, batch=49
2025-07-24 14:10:40,424 [INFO] Evaluating: window=28, layers=1, units=47, dropout=0.131, lr=0.00413, batch=35
2025-07-24 14:10:44,423 [INFO] Evaluating: window=14, layers=1, units=56, dropout=0.363, lr=0.00768, batch=56
2025-07-24 14:10:48,600 [INFO] Evaluating: window=34, layers=2, units=60, dropout=0.157, lr=0.00954, batch=29
2025-07-24 14:10:53,644 [INFO] Evaluating: window=34, layers=2, units=47, dropout=0.157, lr=0.00780, batch=38
2025-07-24 14:10:58,574 [INFO] Evaluating: window=25, layers=1, units=53, dropout=0.197, lr=0.00744, batch=58
2025-07-24 14:11:02,489 [INFO] Evaluating: window=19, layers=1, units=61, dropout=0.252, lr=0.00281, batch=49
2025-07-24 14:11:06,464 [INFO] Evaluating: window=19, layers=2, units=56, dropout=0.363, lr=0.00716, batch=56
2025-07-24 14:11:11,053 [INFO] Evaluating: window=37, layers=1, units=61, dropout=0.251, lr=0.00964, batch=20
2025-07-24 14:11:15,225 [INFO] Evaluating: window=14, layers=2, units=33, dropout=0.199, lr=0.00743, batch=30
2025-07-24 14:13:30,079 [INFO] Saving results to data/tuning_results/jsons/tier1_lstm.json
2025-07-24 14:13:30,080 [INFO] Resuming from 1 completed folds
2025-07-24 14:13:30,092 [INFO] Starting fold 60
2025-07-24 14:13:30,101 [INFO] Fold 60: Volatility=Medium, GA(pop=15, gen=10), BO(calls=12), top_n=4
2025-07-24 14:13:30,110 [INFO] Evaluating: window=21, layers=2, units=55, dropout=0.339, lr=0.00164, batch=23
2025-07-24 14:13:31,796 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.383, lr=0.00030, batch=62
2025-07-24 14:13:33,412 [INFO] Evaluating: window=34, layers=1, units=37, dropout=0.173, lr=0.00311, batch=41
2025-07-24 14:13:34,274 [WARNING] 5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x15f7db880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 14:13:34,329 [WARNING] 6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x15f7db880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 14:13:34,516 [INFO] Evaluating: window=22, layers=1, units=51, dropout=0.156, lr=0.00299, batch=33
2025-07-24 14:13:35,629 [INFO] Evaluating: window=23, layers=2, units=38, dropout=0.306, lr=0.00596, batch=18
2025-07-24 14:13:37,569 [INFO] Evaluating: window=28, layers=1, units=34, dropout=0.480, lr=0.00966, batch=54
2025-07-24 14:13:38,672 [INFO] Evaluating: window=19, layers=1, units=53, dropout=0.276, lr=0.00131, batch=39
2025-07-24 14:13:39,796 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.365, lr=0.00319, batch=40
2025-07-24 14:13:41,381 [INFO] Evaluating: window=26, layers=1, units=63, dropout=0.410, lr=0.00940, batch=58
2025-07-24 14:13:42,591 [INFO] Evaluating: window=27, layers=2, units=34, dropout=0.178, lr=0.00055, batch=31
2025-07-24 14:13:56,287 [INFO] Saving results to data/tuning_results/jsons/tier1_lstm.json
2025-07-24 14:13:56,287 [INFO] Resuming from 1 completed folds
2025-07-24 14:13:56,299 [INFO] Starting fold 60
2025-07-24 14:13:56,306 [INFO] Fold 60: Volatility=Medium, GA(pop=15, gen=10), BO(calls=12), top_n=4
2025-07-24 14:13:56,315 [INFO] Evaluating: window=21, layers=2, units=55, dropout=0.339, lr=0.00164, batch=23
2025-07-24 14:13:57,993 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.383, lr=0.00030, batch=62
2025-07-24 14:13:59,641 [INFO] Evaluating: window=34, layers=1, units=37, dropout=0.173, lr=0.00311, batch=41
2025-07-24 14:14:00,530 [WARNING] 5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3060d7880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 14:14:00,584 [WARNING] 6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3060d7880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 14:14:00,765 [INFO] Evaluating: window=22, layers=1, units=51, dropout=0.156, lr=0.00299, batch=33
2025-07-24 14:14:01,879 [INFO] Evaluating: window=23, layers=2, units=38, dropout=0.306, lr=0.00596, batch=18
2025-07-24 14:14:03,791 [INFO] Evaluating: window=28, layers=1, units=34, dropout=0.480, lr=0.00966, batch=54
2025-07-24 14:14:04,943 [INFO] Evaluating: window=19, layers=1, units=53, dropout=0.276, lr=0.00131, batch=39
2025-07-24 14:18:45,173 [INFO] Saving results to data/tuning_results/jsons/tier1_lstm.json
2025-07-24 14:18:45,173 [INFO] Resuming from 1 completed folds
2025-07-24 14:18:45,184 [INFO] Starting fold 60
2025-07-24 14:18:45,191 [INFO] Fold 60: Volatility=Medium, GA(pop=15, gen=10), BO(calls=12), top_n=4
2025-07-24 14:18:45,200 [INFO] Evaluating: window=21, layers=2, units=55, dropout=0.339, lr=0.00164, batch=23
2025-07-24 14:18:46,939 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.383, lr=0.00030, batch=62
2025-07-24 14:18:48,598 [INFO] Evaluating: window=34, layers=1, units=37, dropout=0.173, lr=0.00311, batch=41
2025-07-24 14:18:49,482 [WARNING] 5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x169dd7d80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 14:18:49,559 [WARNING] 6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x169dd7d80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 14:18:49,741 [INFO] Evaluating: window=22, layers=1, units=51, dropout=0.156, lr=0.00299, batch=33
2025-07-24 14:18:50,855 [INFO] Evaluating: window=23, layers=2, units=38, dropout=0.306, lr=0.00596, batch=18
2025-07-24 14:18:52,784 [INFO] Evaluating: window=28, layers=1, units=34, dropout=0.480, lr=0.00966, batch=54
2025-07-24 14:18:53,946 [INFO] Evaluating: window=19, layers=1, units=53, dropout=0.276, lr=0.00131, batch=39
2025-07-24 14:18:55,078 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.365, lr=0.00319, batch=40
2025-07-24 14:18:56,564 [INFO] Evaluating: window=26, layers=1, units=63, dropout=0.410, lr=0.00940, batch=58
2025-07-24 14:18:57,704 [INFO] Evaluating: window=27, layers=2, units=34, dropout=0.178, lr=0.00055, batch=31
2025-07-24 14:18:59,600 [INFO] Evaluating: window=21, layers=1, units=58, dropout=0.243, lr=0.00288, batch=42
2025-07-24 14:19:00,798 [INFO] Evaluating: window=14, layers=2, units=34, dropout=0.495, lr=0.00775, batch=25
2025-07-24 14:19:02,682 [INFO] Evaluating: window=10, layers=2, units=54, dropout=0.392, lr=0.00774, batch=19
2025-07-24 14:19:24,298 [INFO] Saving results to data/tuning_results/jsons/tier1_lstm.json
2025-07-24 14:19:24,309 [INFO] Starting fold 52
2025-07-24 14:19:24,318 [INFO] Fold 52: Volatility=Medium, GA(pop=15, gen=10), BO(calls=12), top_n=4
2025-07-24 14:19:24,326 [INFO] Evaluating: window=21, layers=2, units=55, dropout=0.339, lr=0.00164, batch=23
2025-07-24 14:19:26,007 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.383, lr=0.00030, batch=62
2025-07-24 14:19:27,627 [INFO] Evaluating: window=34, layers=1, units=37, dropout=0.173, lr=0.00311, batch=41
2025-07-24 14:19:28,519 [WARNING] 5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16ead7880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 14:19:28,572 [WARNING] 6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16ead7880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 14:19:28,754 [INFO] Evaluating: window=22, layers=1, units=51, dropout=0.156, lr=0.00299, batch=33
2025-07-24 14:19:29,886 [INFO] Evaluating: window=23, layers=2, units=38, dropout=0.306, lr=0.00596, batch=18
2025-07-24 14:19:31,782 [INFO] Evaluating: window=28, layers=1, units=34, dropout=0.480, lr=0.00966, batch=54
2025-07-24 14:19:32,890 [INFO] Evaluating: window=19, layers=1, units=53, dropout=0.276, lr=0.00131, batch=39
2025-07-24 14:19:33,994 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.365, lr=0.00319, batch=40
2025-07-24 14:19:35,513 [INFO] Evaluating: window=26, layers=1, units=63, dropout=0.410, lr=0.00940, batch=58
2025-07-24 14:19:36,693 [INFO] Evaluating: window=27, layers=2, units=34, dropout=0.178, lr=0.00055, batch=31
2025-07-24 14:19:38,536 [INFO] Evaluating: window=21, layers=1, units=58, dropout=0.243, lr=0.00288, batch=42
2025-07-24 14:19:39,702 [INFO] Evaluating: window=14, layers=2, units=34, dropout=0.495, lr=0.00775, batch=25
2025-07-24 14:19:41,521 [INFO] Evaluating: window=10, layers=2, units=54, dropout=0.392, lr=0.00774, batch=19
2025-07-24 14:19:43,372 [INFO] Evaluating: window=20, layers=1, units=59, dropout=0.349, lr=0.00338, batch=19
2025-07-24 14:19:44,652 [INFO] Evaluating: window=19, layers=1, units=55, dropout=0.355, lr=0.00888, batch=38
2025-07-24 14:19:45,876 [INFO] Evaluating: window=21, layers=1, units=41, dropout=0.243, lr=0.00321, batch=42
2025-07-24 14:19:47,083 [INFO] Evaluating: window=10, layers=2, units=54, dropout=0.392, lr=0.00777, batch=59
2025-07-24 14:19:48,873 [INFO] Evaluating: window=28, layers=2, units=34, dropout=0.178, lr=0.00055, batch=23
2025-07-24 14:19:50,864 [INFO] Evaluating: window=23, layers=1, units=37, dropout=0.352, lr=0.00599, batch=19
2025-07-24 14:19:52,082 [INFO] Evaluating: window=11, layers=1, units=58, dropout=0.244, lr=0.00288, batch=40
2025-07-24 14:19:53,233 [INFO] Evaluating: window=26, layers=1, units=63, dropout=0.300, lr=0.00913, batch=17
2025-07-24 14:19:54,670 [INFO] Evaluating: window=21, layers=2, units=55, dropout=0.339, lr=0.00741, batch=23
2025-07-24 14:19:56,502 [INFO] Evaluating: window=11, layers=2, units=57, dropout=0.365, lr=0.00286, batch=40
2025-07-24 14:19:58,278 [INFO] Evaluating: window=26, layers=1, units=63, dropout=0.410, lr=0.00957, batch=19
2025-07-24 14:19:59,727 [INFO] Evaluating: window=21, layers=2, units=55, dropout=0.339, lr=0.00164, batch=32
2025-07-24 14:20:01,766 [INFO] Evaluating: window=20, layers=2, units=60, dropout=0.290, lr=0.00335, batch=18
2025-07-24 14:20:03,976 [INFO] Evaluating: window=21, layers=2, units=39, dropout=0.377, lr=0.00319, batch=42
2025-07-24 14:20:06,080 [INFO] Evaluating: window=23, layers=2, units=38, dropout=0.415, lr=0.00596, batch=60
2025-07-24 14:20:08,151 [INFO] Evaluating: window=10, layers=2, units=49, dropout=0.365, lr=0.00197, batch=19
2025-07-24 14:20:10,274 [INFO] Evaluating: window=27, layers=2, units=34, dropout=0.137, lr=0.00055, batch=31
2025-07-24 14:20:12,438 [INFO] Evaluating: window=27, layers=2, units=34, dropout=0.375, lr=0.00055, batch=31
2025-07-24 14:20:14,643 [INFO] Evaluating: window=10, layers=1, units=58, dropout=0.249, lr=0.00321, batch=42
2025-07-24 14:20:16,124 [INFO] Evaluating: window=10, layers=2, units=55, dropout=0.340, lr=0.00775, batch=21
2025-07-24 14:20:18,404 [INFO] Evaluating: window=23, layers=2, units=38, dropout=0.304, lr=0.00285, batch=18
2025-07-24 14:20:20,742 [INFO] Evaluating: window=28, layers=2, units=32, dropout=0.154, lr=0.00817, batch=25
2025-07-24 14:20:23,016 [INFO] Evaluating: window=10, layers=2, units=54, dropout=0.339, lr=0.00742, batch=19
2025-07-24 14:20:25,277 [INFO] Evaluating: window=23, layers=2, units=38, dropout=0.245, lr=0.00230, batch=18
2025-07-24 14:20:27,649 [INFO] Evaluating: window=11, layers=2, units=57, dropout=0.416, lr=0.00624, batch=40
2025-07-24 14:20:29,680 [INFO] Evaluating: window=21, layers=2, units=39, dropout=0.157, lr=0.00319, batch=42
2025-07-24 14:20:31,945 [INFO] Evaluating: window=21, layers=2, units=40, dropout=0.365, lr=0.00301, batch=40
2025-07-24 14:20:34,301 [INFO] Evaluating: window=22, layers=2, units=54, dropout=0.391, lr=0.00743, batch=59
2025-07-24 14:20:36,629 [INFO] Evaluating: window=11, layers=2, units=56, dropout=0.360, lr=0.00598, batch=40
2025-07-24 14:20:38,768 [INFO] Evaluating: window=10, layers=2, units=56, dropout=0.391, lr=0.00029, batch=57
2025-07-24 14:20:41,046 [INFO] Evaluating: window=21, layers=2, units=55, dropout=0.392, lr=0.00773, batch=23
2025-07-24 14:20:43,360 [INFO] Evaluating: window=21, layers=1, units=58, dropout=0.303, lr=0.00619, batch=42
2025-07-24 14:20:45,085 [INFO] Evaluating: window=10, layers=2, units=55, dropout=0.392, lr=0.00773, batch=18
2025-07-24 14:20:47,516 [INFO] Evaluating: window=11, layers=2, units=38, dropout=0.417, lr=0.00277, batch=40
2025-07-24 14:20:49,688 [INFO] Evaluating: window=21, layers=2, units=55, dropout=0.365, lr=0.00650, batch=22
2025-07-24 14:20:52,243 [INFO] Evaluating: window=21, layers=2, units=40, dropout=0.365, lr=0.00301, batch=40
2025-07-24 14:21:09,796 [INFO] Saving results to data/tuning_results/jsons/tier1_lstm.json
2025-07-24 14:21:09,807 [INFO] Starting fold 52
2025-07-24 14:21:09,815 [INFO] Fold 52: Volatility=Medium, GA(pop=15, gen=10), BO(calls=12), top_n=4
2025-07-24 14:21:09,823 [INFO] Evaluating: window=21, layers=2, units=55, dropout=0.339, lr=0.00164, batch=23
2025-07-24 14:21:11,548 [INFO] Evaluating: window=11, layers=2, units=51, dropout=0.383, lr=0.00030, batch=62
2025-07-24 14:21:13,185 [INFO] Evaluating: window=34, layers=1, units=37, dropout=0.173, lr=0.00311, batch=41
2025-07-24 14:21:14,064 [WARNING] 5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x1699d7880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 14:21:14,119 [WARNING] 6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x1699d7880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
2025-07-24 14:21:14,303 [INFO] Evaluating: window=22, layers=1, units=51, dropout=0.156, lr=0.00299, batch=33
2025-07-24 14:21:15,414 [INFO] Evaluating: window=23, layers=2, units=38, dropout=0.306, lr=0.00596, batch=18
2025-07-24 14:21:17,365 [INFO] Evaluating: window=28, layers=1, units=34, dropout=0.480, lr=0.00966, batch=54
2025-07-24 14:21:18,482 [INFO] Evaluating: window=19, layers=1, units=53, dropout=0.276, lr=0.00131, batch=39
2025-07-24 14:21:19,620 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.365, lr=0.00319, batch=40
2025-07-24 14:21:21,209 [INFO] Evaluating: window=26, layers=1, units=63, dropout=0.410, lr=0.00940, batch=58
2025-07-24 14:21:22,395 [INFO] Evaluating: window=27, layers=2, units=34, dropout=0.178, lr=0.00055, batch=31
2025-07-24 14:21:24,291 [INFO] Evaluating: window=21, layers=1, units=58, dropout=0.243, lr=0.00288, batch=42
2025-07-24 14:21:25,495 [INFO] Evaluating: window=14, layers=2, units=34, dropout=0.495, lr=0.00775, batch=25
2025-07-24 14:21:27,355 [INFO] Evaluating: window=10, layers=2, units=54, dropout=0.392, lr=0.00774, batch=19
2025-07-24 14:21:29,282 [INFO] Evaluating: window=20, layers=1, units=59, dropout=0.349, lr=0.00338, batch=19
2025-07-24 14:21:30,613 [INFO] Evaluating: window=19, layers=1, units=55, dropout=0.355, lr=0.00888, batch=38
2025-07-24 14:21:31,904 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.383, lr=0.00337, batch=62
2025-07-24 14:21:33,733 [INFO] Evaluating: window=14, layers=2, units=34, dropout=0.495, lr=0.00778, batch=25
2025-07-24 14:21:35,672 [INFO] Evaluating: window=19, layers=2, units=53, dropout=0.276, lr=0.00131, batch=39
2025-07-24 14:21:37,610 [INFO] Evaluating: window=34, layers=1, units=37, dropout=0.166, lr=0.00308, batch=41
2025-07-24 14:21:38,910 [INFO] Evaluating: window=19, layers=1, units=55, dropout=0.349, lr=0.00856, batch=39
2025-07-24 14:21:40,185 [INFO] Evaluating: window=26, layers=1, units=63, dropout=0.415, lr=0.00940, batch=60
2025-07-24 14:21:41,488 [INFO] Evaluating: window=35, layers=1, units=53, dropout=0.276, lr=0.00302, batch=39
2025-07-24 14:21:42,851 [INFO] Evaluating: window=14, layers=2, units=34, dropout=0.495, lr=0.00052, batch=25
2025-07-24 14:21:44,800 [INFO] Evaluating: window=11, layers=2, units=50, dropout=0.365, lr=0.00051, batch=40
2025-07-24 14:21:46,588 [INFO] Evaluating: window=21, layers=1, units=63, dropout=0.410, lr=0.00937, batch=56
2025-07-24 14:21:47,994 [INFO] Evaluating: window=18, layers=1, units=55, dropout=0.294, lr=0.00164, batch=23
2025-07-24 14:21:49,425 [INFO] Evaluating: window=24, layers=2, units=40, dropout=0.323, lr=0.00599, batch=18
2025-07-24 14:21:51,624 [INFO] Evaluating: window=20, layers=1, units=59, dropout=0.355, lr=0.00338, batch=18
2025-07-24 14:21:53,109 [INFO] Evaluating: window=23, layers=2, units=38, dropout=0.270, lr=0.00596, batch=17
2025-07-24 14:21:55,373 [INFO] Evaluating: window=18, layers=1, units=37, dropout=0.173, lr=0.00140, batch=41
2025-07-24 14:21:56,782 [INFO] Evaluating: window=14, layers=2, units=34, dropout=0.155, lr=0.00736, batch=31
2025-07-24 14:21:58,853 [INFO] Evaluating: window=10, layers=1, units=37, dropout=0.174, lr=0.00145, batch=41
2025-07-24 14:22:00,279 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.384, lr=0.00337, batch=56
2025-07-24 14:22:02,319 [INFO] Evaluating: window=34, layers=1, units=36, dropout=0.167, lr=0.00311, batch=20
2025-07-24 14:22:03,896 [INFO] Evaluating: window=33, layers=2, units=38, dropout=0.365, lr=0.00049, batch=40
2025-07-24 14:22:06,076 [INFO] Evaluating: window=23, layers=2, units=34, dropout=0.499, lr=0.00596, batch=25
2025-07-24 14:22:08,318 [INFO] Evaluating: window=19, layers=2, units=40, dropout=0.391, lr=0.00337, batch=62
2025-07-24 14:22:10,452 [INFO] Evaluating: window=27, layers=2, units=34, dropout=0.495, lr=0.00094, batch=25
2025-07-24 14:22:12,737 [INFO] Evaluating: window=18, layers=2, units=50, dropout=0.364, lr=0.00046, batch=40
2025-07-24 14:22:14,938 [INFO] Evaluating: window=21, layers=1, units=61, dropout=0.364, lr=0.00936, batch=61
2025-07-24 14:22:16,509 [INFO] Evaluating: window=20, layers=1, units=58, dropout=0.338, lr=0.00338, batch=39
2025-07-24 14:22:18,096 [INFO] Evaluating: window=12, layers=1, units=50, dropout=0.173, lr=0.00314, batch=41
2025-07-24 14:22:19,654 [INFO] Evaluating: window=14, layers=2, units=38, dropout=0.251, lr=0.00725, batch=17
2025-07-24 14:22:21,957 [INFO] Evaluating: window=11, layers=1, units=35, dropout=0.148, lr=0.00137, batch=39
2025-07-24 14:22:23,552 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.180, lr=0.00337, batch=63
2025-07-24 14:22:25,738 [INFO] Evaluating: window=27, layers=2, units=38, dropout=0.364, lr=0.00094, batch=40
2025-07-24 14:22:28,053 [INFO] Evaluating: window=19, layers=2, units=54, dropout=0.276, lr=0.00131, batch=39
2025-07-24 14:22:30,373 [INFO] Evaluating: window=11, layers=1, units=40, dropout=0.383, lr=0.00394, batch=40
2025-07-24 14:22:31,924 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.383, lr=0.00051, batch=40
2025-07-24 14:22:34,025 [INFO] Evaluating: window=33, layers=2, units=38, dropout=0.363, lr=0.00066, batch=39
2025-07-24 14:22:36,437 [INFO] Evaluating: window=34, layers=1, units=37, dropout=0.411, lr=0.00311, batch=56
2025-07-24 14:22:38,157 [INFO] Evaluating: window=23, layers=2, units=34, dropout=0.478, lr=0.00590, batch=25
2025-07-24 14:22:40,611 [INFO] Evaluating: window=19, layers=1, units=53, dropout=0.178, lr=0.00131, batch=38
2025-07-24 14:22:42,391 [INFO] Evaluating: window=34, layers=2, units=34, dropout=0.495, lr=0.00049, batch=25
2025-07-24 14:22:44,923 [INFO] Evaluating: window=19, layers=1, units=53, dropout=0.244, lr=0.00131, batch=39
2025-07-24 14:22:46,678 [INFO] Evaluating: window=35, layers=2, units=37, dropout=0.157, lr=0.00311, batch=63
2025-07-24 14:22:49,125 [INFO] Evaluating: window=10, layers=2, units=51, dropout=0.366, lr=0.00337, batch=62
2025-07-24 14:22:51,489 [INFO] Evaluating: window=19, layers=2, units=53, dropout=0.278, lr=0.00133, batch=40
2025-07-24 14:22:53,990 [INFO] Evaluating: window=21, layers=1, units=63, dropout=0.176, lr=0.00929, batch=40
2025-07-24 14:22:55,824 [INFO] Evaluating: window=27, layers=2, units=34, dropout=0.499, lr=0.00100, batch=27
2025-07-24 14:22:58,434 [INFO] Evaluating: window=19, layers=2, units=53, dropout=0.276, lr=0.00131, batch=55
2025-07-24 14:23:01,036 [INFO] Evaluating: window=20, layers=1, units=36, dropout=0.153, lr=0.00140, batch=41
2025-07-24 14:23:02,898 [INFO] Evaluating: window=11, layers=2, units=50, dropout=0.365, lr=0.00051, batch=41
2025-07-24 14:23:05,366 [INFO] Evaluating: window=32, layers=2, units=38, dropout=0.276, lr=0.00131, batch=39
2025-07-24 14:23:07,963 [INFO] Evaluating: window=19, layers=2, units=40, dropout=0.336, lr=0.00047, batch=42
2025-07-24 14:23:10,520 [INFO] Evaluating: window=21, layers=2, units=34, dropout=0.495, lr=0.00094, batch=25
2025-07-24 14:23:13,155 [INFO] Evaluating: window=19, layers=2, units=53, dropout=0.276, lr=0.00091, batch=39
2025-07-24 14:23:15,781 [INFO] Evaluating: window=19, layers=2, units=53, dropout=0.304, lr=0.00246, batch=56
2025-07-24 14:23:18,361 [INFO] Evaluating: window=21, layers=1, units=63, dropout=0.410, lr=0.00937, batch=40
2025-07-24 14:23:20,353 [INFO] Evaluating: window=34, layers=1, units=37, dropout=0.173, lr=0.00311, batch=41
2025-07-24 14:23:22,420 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.383, lr=0.00337, batch=63
2025-07-24 14:23:24,976 [INFO] Evaluating: window=19, layers=2, units=55, dropout=0.365, lr=0.00049, batch=40
2025-07-24 14:23:27,633 [INFO] Evaluating: window=27, layers=2, units=56, dropout=0.276, lr=0.00133, batch=39
2025-07-24 14:23:30,385 [INFO] Evaluating: window=27, layers=1, units=61, dropout=0.176, lr=0.00977, batch=40
2025-07-24 14:23:32,464 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.383, lr=0.00342, batch=62
2025-07-24 14:23:35,059 [INFO] Evaluating: window=15, layers=1, units=37, dropout=0.173, lr=0.00127, batch=41
2025-07-24 14:23:37,097 [INFO] Evaluating: window=11, layers=2, units=54, dropout=0.385, lr=0.00131, batch=39
2025-07-24 14:23:39,779 [INFO] Evaluating: window=21, layers=1, units=61, dropout=0.136, lr=0.00929, batch=40
2025-07-24 14:23:41,887 [INFO] Evaluating: window=22, layers=1, units=63, dropout=0.412, lr=0.00937, batch=55
2025-07-24 14:23:43,999 [INFO] Evaluating: window=33, layers=2, units=38, dropout=0.364, lr=0.00065, batch=40
2025-07-24 14:23:46,810 [INFO] Evaluating: window=21, layers=1, units=63, dropout=0.382, lr=0.00904, batch=56
2025-07-24 14:23:48,945 [INFO] Evaluating: window=10, layers=2, units=50, dropout=0.365, lr=0.00052, batch=39
2025-07-24 14:23:51,646 [INFO] Evaluating: window=19, layers=2, units=53, dropout=0.276, lr=0.00131, batch=39
2025-07-24 14:23:54,501 [INFO] Evaluating: window=34, layers=1, units=37, dropout=0.173, lr=0.00325, batch=41
2025-07-24 14:23:56,670 [INFO] Evaluating: window=19, layers=2, units=40, dropout=0.255, lr=0.00337, batch=58
2025-07-24 14:23:59,283 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.383, lr=0.00271, batch=62
2025-07-24 14:24:02,014 [INFO] Evaluating: window=27, layers=2, units=34, dropout=0.493, lr=0.00094, batch=25
2025-07-24 14:24:04,956 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.385, lr=0.00326, batch=62
2025-07-24 14:24:07,734 [INFO] Evaluating: window=11, layers=2, units=41, dropout=0.411, lr=0.00369, batch=62
2025-07-24 14:24:10,608 [INFO] Evaluating: window=19, layers=2, units=54, dropout=0.276, lr=0.00130, batch=41
2025-07-24 14:24:13,526 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.364, lr=0.00342, batch=62
2025-07-24 14:24:16,353 [INFO] Evaluating: window=27, layers=2, units=35, dropout=0.495, lr=0.00119, batch=38
2025-07-24 14:24:19,317 [INFO] Evaluating: window=19, layers=2, units=53, dropout=0.330, lr=0.00146, batch=39
2025-07-24 14:24:22,287 [INFO] Evaluating: window=11, layers=1, units=61, dropout=0.383, lr=0.00958, batch=55
2025-07-24 14:24:24,585 [INFO] Evaluating: window=11, layers=2, units=37, dropout=0.173, lr=0.00342, batch=62
2025-07-24 14:24:27,482 [INFO] Evaluating: window=18, layers=2, units=54, dropout=0.276, lr=0.00353, batch=39
2025-07-24 14:24:30,481 [INFO] Evaluating: window=18, layers=1, units=62, dropout=0.176, lr=0.00980, batch=40
2025-07-24 14:24:32,852 [INFO] Evaluating: window=27, layers=2, units=56, dropout=0.273, lr=0.00131, batch=25
2025-07-24 14:24:36,002 [INFO] Evaluating: window=19, layers=2, units=55, dropout=0.276, lr=0.00166, batch=39
2025-07-24 14:24:39,046 [INFO] Evaluating: window=21, layers=2, units=40, dropout=0.410, lr=0.00349, batch=63
2025-07-24 14:24:42,066 [INFO] Evaluating: window=33, layers=2, units=38, dropout=0.365, lr=0.00049, batch=40
2025-07-24 14:24:45,238 [INFO] Evaluating: window=34, layers=1, units=40, dropout=0.384, lr=0.00311, batch=41
2025-07-24 14:24:47,714 [INFO] Evaluating: window=12, layers=2, units=40, dropout=0.383, lr=0.00115, batch=62
2025-07-24 14:24:50,715 [INFO] Evaluating: window=21, layers=1, units=38, dropout=0.173, lr=0.00089, batch=41
2025-07-24 14:24:53,174 [INFO] Evaluating: window=19, layers=2, units=40, dropout=0.279, lr=0.00136, batch=62
2025-07-24 14:24:56,248 [INFO] Evaluating: window=11, layers=2, units=42, dropout=0.383, lr=0.00349, batch=55
2025-07-24 14:24:59,300 [INFO] Evaluating: window=21, layers=2, units=38, dropout=0.410, lr=0.00937, batch=56
2025-07-24 14:25:02,432 [INFO] Evaluating: window=11, layers=2, units=54, dropout=0.276, lr=0.00131, batch=61
2025-07-24 14:25:05,526 [INFO] Evaluating: window=19, layers=1, units=37, dropout=0.172, lr=0.00313, batch=39
2025-07-24 14:25:08,068 [INFO] Evaluating: window=18, layers=2, units=39, dropout=0.329, lr=0.00146, batch=38
2025-07-24 14:25:11,247 [INFO] Evaluating: window=18, layers=1, units=53, dropout=0.276, lr=0.00131, batch=39
2025-07-24 14:25:13,811 [INFO] Evaluating: window=19, layers=2, units=40, dropout=0.252, lr=0.00342, batch=62
2025-07-24 14:25:16,984 [INFO] Evaluating: window=33, layers=2, units=38, dropout=0.365, lr=0.00049, batch=42
2025-07-24 14:25:20,485 [INFO] Evaluating: window=11, layers=1, units=61, dropout=0.383, lr=0.00985, batch=62
2025-07-24 14:25:23,082 [INFO] Evaluating: window=21, layers=1, units=62, dropout=0.410, lr=0.00343, batch=63
2025-07-24 14:25:25,700 [INFO] Evaluating: window=19, layers=2, units=41, dropout=0.383, lr=0.00342, batch=41
2025-07-24 14:25:28,959 [INFO] Evaluating: window=34, layers=2, units=55, dropout=0.276, lr=0.00129, batch=41
2025-07-24 14:25:32,388 [INFO] Evaluating: window=21, layers=2, units=54, dropout=0.410, lr=0.00349, batch=63
2025-07-24 14:25:35,685 [INFO] Evaluating: window=34, layers=2, units=37, dropout=0.173, lr=0.00311, batch=41
2025-07-24 14:25:39,072 [INFO] Evaluating: window=11, layers=2, units=54, dropout=0.388, lr=0.00078, batch=39
2025-07-24 14:25:42,366 [INFO] Evaluating: window=18, layers=2, units=38, dropout=0.365, lr=0.00049, batch=40
2025-07-24 14:25:45,698 [INFO] Evaluating: window=19, layers=2, units=54, dropout=0.276, lr=0.00146, batch=39
2025-07-24 14:25:49,059 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.383, lr=0.00337, batch=63
2025-07-24 14:25:52,352 [INFO] Evaluating: window=10, layers=2, units=40, dropout=0.410, lr=0.00381, batch=63
2025-07-24 14:25:55,688 [INFO] Evaluating: window=19, layers=2, units=54, dropout=0.242, lr=0.00131, batch=39
2025-07-24 14:25:59,109 [INFO] Evaluating: window=11, layers=1, units=39, dropout=0.410, lr=0.00937, batch=56
2025-07-24 14:26:01,846 [INFO] Evaluating: window=21, layers=2, units=40, dropout=0.383, lr=0.00349, batch=63
2025-07-24 14:26:05,246 [INFO] Evaluating: window=23, layers=2, units=40, dropout=0.410, lr=0.00349, batch=63
2025-07-24 14:26:08,681 [INFO] Evaluating: window=34, layers=2, units=40, dropout=0.252, lr=0.00342, batch=62
2025-07-24 14:26:12,179 [INFO] Evaluating: window=15, layers=2, units=53, dropout=0.330, lr=0.00131, batch=39
2025-07-24 14:26:15,760 [INFO] Evaluating: window=21, layers=2, units=54, dropout=0.484, lr=0.00349, batch=62
2025-07-24 14:26:19,309 [INFO] Evaluating: window=22, layers=2, units=54, dropout=0.388, lr=0.00047, batch=39
2025-07-24 14:26:22,841 [INFO] Evaluating: window=20, layers=2, units=53, dropout=0.276, lr=0.00131, batch=39
2025-07-24 14:26:26,395 [INFO] Evaluating: window=21, layers=2, units=63, dropout=0.383, lr=0.00244, batch=62
2025-07-24 14:26:29,950 [INFO] Evaluating: window=13, layers=2, units=40, dropout=0.410, lr=0.00342, batch=62
2025-07-24 14:26:33,433 [INFO] Evaluating: window=19, layers=2, units=40, dropout=0.383, lr=0.00337, batch=63
2025-07-24 14:26:36,978 [INFO] Evaluating: window=19, layers=1, units=63, dropout=0.252, lr=0.00993, batch=55
2025-07-24 14:26:39,935 [INFO] Evaluating: window=33, layers=2, units=38, dropout=0.365, lr=0.00049, batch=40
2025-07-24 14:26:43,603 [INFO] Evaluating: window=21, layers=2, units=40, dropout=0.410, lr=0.00348, batch=63
2025-07-24 14:26:47,183 [INFO] Evaluating: window=11, layers=2, units=54, dropout=0.387, lr=0.00078, batch=38
2025-07-24 14:26:50,795 [INFO] Evaluating: window=21, layers=1, units=63, dropout=0.410, lr=0.00937, batch=38
2025-07-24 14:26:53,882 [INFO] Evaluating: window=33, layers=2, units=38, dropout=0.365, lr=0.00049, batch=40
2025-07-24 14:26:57,591 [INFO] Evaluating: window=21, layers=2, units=40, dropout=0.411, lr=0.00343, batch=63
2025-07-24 14:27:01,237 [INFO] Evaluating: window=11, layers=2, units=54, dropout=0.276, lr=0.00131, batch=38
2025-07-24 14:27:04,904 [INFO] Evaluating: window=21, layers=2, units=39, dropout=0.410, lr=0.00257, batch=62
2025-07-24 14:27:08,587 [INFO] Evaluating: window=11, layers=2, units=40, dropout=0.384, lr=0.00342, batch=62
2025-07-24 14:27:12,226 [INFO] Evaluating: window=11, layers=2, units=54, dropout=0.388, lr=0.00079, batch=39
2025-07-24 14:27:15,944 [INFO] Evaluating: window=19, layers=2, units=53, dropout=0.277, lr=0.00131, batch=39
2025-07-24 14:27:19,720 [INFO] Evaluating: window=19, layers=2, units=57, dropout=0.276, lr=0.00131, batch=56
2025-07-24 14:27:23,501 [INFO] Evaluating: window=22, layers=2, units=54, dropout=0.410, lr=0.00349, batch=63
2025-07-24 14:27:27,351 [INFO] Fold 52: BO(n_calls=12)
2025-07-24 14:27:27,359 [INFO] Evaluating: window=34, layers=1, units=57, dropout=0.339, lr=0.00078, batch=21
2025-07-24 14:27:30,746 [INFO] Evaluating: window=24, layers=2, units=37, dropout=0.360, lr=0.00013, batch=51
2025-07-24 14:27:34,540 [INFO] Evaluating: window=38, layers=1, units=64, dropout=0.347, lr=0.00167, batch=16
2025-07-24 14:27:37,942 [INFO] Evaluating: window=11, layers=2, units=45, dropout=0.119, lr=0.00886, batch=27
2025-07-24 14:27:41,747 [INFO] Evaluating: window=13, layers=2, units=44, dropout=0.493, lr=0.00086, batch=57
2025-07-24 14:27:45,533 [INFO] Evaluating: window=30, layers=2, units=32, dropout=0.477, lr=0.00134, batch=34
2025-07-24 14:27:49,463 [INFO] Evaluating: window=10, layers=1, units=40, dropout=0.373, lr=0.00166, batch=56
2025-07-24 14:27:52,657 [INFO] Evaluating: window=15, layers=2, units=38, dropout=0.402, lr=0.00071, batch=26
2025-07-24 14:27:56,572 [INFO] Evaluating: window=27, layers=1, units=59, dropout=0.280, lr=0.00062, batch=60
2025-07-24 14:27:59,978 [INFO] Evaluating: window=32, layers=2, units=50, dropout=0.308, lr=0.00836, batch=57
2025-07-24 14:28:04,026 [INFO] Evaluating: window=10, layers=3, units=64, dropout=0.100, lr=0.00010, batch=60
2025-07-24 14:28:08,522 [INFO] Evaluating: window=18, layers=3, units=64, dropout=0.203, lr=0.01000, batch=16
2025-07-24 14:28:13,459 [INFO] â†’ Fold 52 done in 423.6s | RMSE=0.4360
2025-07-24 14:28:13,459 [INFO] Starting fold 60
2025-07-24 14:28:13,464 [INFO] Fold 60: Volatility=Medium, GA(pop=15, gen=10), BO(calls=12), top_n=4
2025-07-24 14:28:13,465 [INFO] Evaluating: window=10, layers=2, units=48, dropout=0.353, lr=0.00648, batch=55
2025-07-24 14:28:17,269 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.417, lr=0.00991, batch=25
2025-07-24 14:35:26,712 [INFO] Evaluating: window=39, layers=1, units=32, dropout=0.118, lr=0.00348, batch=46
2025-07-24 14:35:30,153 [INFO] Evaluating: window=12, layers=1, units=61, dropout=0.429, lr=0.00811, batch=58
2025-07-24 14:35:33,387 [INFO] Evaluating: window=31, layers=1, units=44, dropout=0.389, lr=0.00288, batch=48
2025-07-24 14:35:36,673 [INFO] Evaluating: window=10, layers=1, units=49, dropout=0.353, lr=0.00581, batch=49
2025-07-24 14:35:39,896 [INFO] Evaluating: window=10, layers=1, units=59, dropout=0.373, lr=0.00122, batch=56
2025-07-24 14:35:43,123 [INFO] Evaluating: window=17, layers=1, units=36, dropout=0.384, lr=0.00670, batch=26
2025-07-24 14:35:46,318 [INFO] Evaluating: window=35, layers=1, units=41, dropout=0.161, lr=0.00274, batch=17
2025-07-24 14:35:49,722 [INFO] Evaluating: window=27, layers=2, units=49, dropout=0.188, lr=0.00043, batch=26
2025-07-24 14:35:53,769 [INFO] Evaluating: window=22, layers=2, units=38, dropout=0.388, lr=0.00868, batch=37
2025-07-24 14:35:57,733 [INFO] Evaluating: window=34, layers=2, units=52, dropout=0.311, lr=0.00469, batch=27
2025-07-24 14:36:01,876 [INFO] Evaluating: window=19, layers=1, units=50, dropout=0.148, lr=0.00918, batch=50
2025-07-24 14:36:05,238 [INFO] Evaluating: window=22, layers=1, units=60, dropout=0.430, lr=0.00725, batch=29
2025-07-24 14:36:08,629 [INFO] Evaluating: window=22, layers=1, units=38, dropout=0.413, lr=0.00718, batch=44
2025-07-24 14:36:11,984 [INFO] Evaluating: window=34, layers=1, units=52, dropout=0.311, lr=0.00469, batch=27
2025-07-24 14:36:15,613 [INFO] Evaluating: window=22, layers=1, units=38, dropout=0.388, lr=0.00868, batch=38
2025-07-24 14:36:19,008 [INFO] Evaluating: window=10, layers=1, units=49, dropout=0.353, lr=0.00581, batch=24
2025-07-24 14:36:22,387 [INFO] Evaluating: window=10, layers=1, units=59, dropout=0.373, lr=0.00122, batch=56
2025-07-24 14:36:25,743 [INFO] Evaluating: window=24, layers=1, units=38, dropout=0.390, lr=0.00289, batch=44
2025-07-24 14:36:29,138 [INFO] Evaluating: window=18, layers=2, units=49, dropout=0.189, lr=0.00043, batch=26
2025-07-24 14:36:33,220 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.417, lr=0.00990, batch=25
2025-07-24 14:36:37,344 [INFO] Evaluating: window=21, layers=2, units=37, dropout=0.353, lr=0.00648, batch=44
2025-07-24 14:36:41,407 [INFO] Evaluating: window=24, layers=2, units=60, dropout=0.430, lr=0.00725, batch=26
2025-07-24 14:36:45,632 [INFO] Evaluating: window=31, layers=2, units=44, dropout=0.389, lr=0.00288, batch=48
2025-07-24 14:36:49,779 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.417, lr=0.01000, batch=55
2025-07-24 14:36:53,886 [INFO] Evaluating: window=10, layers=2, units=49, dropout=0.353, lr=0.00648, batch=55
2025-07-24 14:36:57,948 [INFO] Evaluating: window=31, layers=1, units=44, dropout=0.412, lr=0.00716, batch=48
2025-07-24 14:37:01,493 [INFO] Evaluating: window=34, layers=2, units=52, dropout=0.309, lr=0.00469, batch=27
2025-07-24 14:37:05,856 [INFO] Evaluating: window=35, layers=1, units=41, dropout=0.161, lr=0.00259, batch=17
2025-07-24 14:37:09,533 [INFO] Evaluating: window=10, layers=1, units=48, dropout=0.353, lr=0.00652, batch=38
2025-07-24 14:37:13,052 [INFO] Evaluating: window=19, layers=2, units=48, dropout=0.417, lr=0.00986, batch=25
2025-07-24 14:37:17,318 [INFO] Evaluating: window=22, layers=2, units=36, dropout=0.431, lr=0.00871, batch=26
2025-07-24 14:37:21,583 [INFO] Evaluating: window=31, layers=2, units=49, dropout=0.145, lr=0.01000, batch=55
2025-07-24 14:37:25,855 [INFO] Evaluating: window=20, layers=2, units=52, dropout=0.422, lr=0.00991, batch=24
2025-07-24 14:37:30,294 [INFO] Evaluating: window=10, layers=1, units=49, dropout=0.353, lr=0.00581, batch=23
2025-07-24 14:37:33,911 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.164, lr=0.00990, batch=25
2025-07-24 14:37:38,278 [INFO] Evaluating: window=22, layers=2, units=41, dropout=0.388, lr=0.00855, batch=25
2025-07-24 14:37:42,654 [INFO] Evaluating: window=22, layers=2, units=38, dropout=0.388, lr=0.00865, batch=58
2025-07-24 14:37:46,903 [INFO] Evaluating: window=34, layers=2, units=52, dropout=0.309, lr=0.00468, batch=27
2025-07-24 14:37:51,400 [INFO] Evaluating: window=24, layers=2, units=61, dropout=0.386, lr=0.00722, batch=37
2025-07-24 14:37:55,808 [INFO] Evaluating: window=20, layers=1, units=39, dropout=0.441, lr=0.00259, batch=17
2025-07-24 14:37:59,582 [INFO] Evaluating: window=38, layers=2, units=48, dropout=0.304, lr=0.00469, batch=23
2025-07-24 14:38:04,178 [INFO] Evaluating: window=10, layers=2, units=48, dropout=0.353, lr=0.00648, batch=55
2025-07-24 14:38:08,485 [INFO] Evaluating: window=35, layers=1, units=41, dropout=0.414, lr=0.00088, batch=16
2025-07-24 14:38:12,369 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.413, lr=0.00979, batch=25
2025-07-24 14:38:16,846 [INFO] Evaluating: window=19, layers=2, units=48, dropout=0.417, lr=0.00986, batch=25
2025-07-24 14:38:21,449 [INFO] Evaluating: window=19, layers=2, units=48, dropout=0.417, lr=0.00991, batch=25
2025-07-24 14:53:26,873 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.353, lr=0.00990, batch=24
2025-07-24 14:53:32,970 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.167, lr=0.00990, batch=25
2025-07-24 14:53:37,824 [INFO] Evaluating: window=34, layers=2, units=36, dropout=0.431, lr=0.00425, batch=27
2025-07-24 14:53:43,386 [INFO] Evaluating: window=36, layers=1, units=41, dropout=0.158, lr=0.00317, batch=25
2025-07-24 14:53:47,496 [INFO] Evaluating: window=35, layers=1, units=41, dropout=0.165, lr=0.00322, batch=17
2025-07-24 14:53:51,720 [INFO] Evaluating: window=35, layers=2, units=52, dropout=0.309, lr=0.00469, batch=27
2025-07-24 14:53:56,541 [INFO] Evaluating: window=23, layers=2, units=36, dropout=0.431, lr=0.00871, batch=26
2025-07-24 14:54:01,465 [INFO] Evaluating: window=10, layers=2, units=49, dropout=0.167, lr=0.00588, batch=56
2025-07-24 14:54:06,176 [INFO] Evaluating: window=20, layers=2, units=52, dropout=0.423, lr=0.00991, batch=24
2025-07-24 14:54:11,240 [INFO] Evaluating: window=22, layers=2, units=52, dropout=0.309, lr=0.00868, batch=26
2025-07-24 14:54:16,215 [INFO] Evaluating: window=19, layers=2, units=48, dropout=0.420, lr=0.00990, batch=17
2025-07-24 14:54:21,225 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.258, lr=0.00981, batch=25
2025-07-24 14:54:26,250 [INFO] Evaluating: window=10, layers=2, units=48, dropout=0.353, lr=0.00647, batch=55
2025-07-24 14:54:31,095 [INFO] Evaluating: window=34, layers=2, units=36, dropout=0.431, lr=0.00385, batch=27
2025-07-24 14:54:36,159 [INFO] Evaluating: window=34, layers=2, units=51, dropout=0.310, lr=0.00499, batch=27
2025-07-24 14:54:41,369 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.413, lr=0.00987, batch=25
2025-07-24 14:54:46,385 [INFO] Evaluating: window=10, layers=2, units=48, dropout=0.353, lr=0.00648, batch=55
2025-07-24 14:54:51,313 [INFO] Evaluating: window=22, layers=2, units=36, dropout=0.431, lr=0.00860, batch=26
2025-07-24 14:54:56,517 [INFO] Evaluating: window=30, layers=1, units=44, dropout=0.431, lr=0.00288, batch=48
2025-07-24 14:55:00,974 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.164, lr=0.00990, batch=25
2025-07-24 14:55:06,110 [INFO] Evaluating: window=35, layers=1, units=41, dropout=0.161, lr=0.00259, batch=17
2025-07-24 14:55:10,730 [INFO] Evaluating: window=20, layers=2, units=49, dropout=0.416, lr=0.00955, batch=25
2025-07-24 14:55:15,865 [INFO] Evaluating: window=37, layers=1, units=41, dropout=0.161, lr=0.00235, batch=16
2025-07-24 14:55:20,368 [INFO] Evaluating: window=21, layers=2, units=48, dropout=0.164, lr=0.00990, batch=25
2025-07-24 14:55:25,612 [INFO] Evaluating: window=34, layers=2, units=52, dropout=0.309, lr=0.00478, batch=27
2025-07-24 14:55:31,095 [INFO] Evaluating: window=22, layers=2, units=36, dropout=0.386, lr=0.00871, batch=26
2025-07-24 14:55:36,268 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.413, lr=0.00979, batch=29
2025-07-24 14:55:41,401 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.417, lr=0.00991, batch=27
2025-07-24 14:55:46,598 [INFO] Evaluating: window=10, layers=2, units=36, dropout=0.361, lr=0.00878, batch=55
2025-07-24 14:55:51,702 [INFO] Evaluating: window=11, layers=2, units=48, dropout=0.164, lr=0.00973, batch=56
2025-07-24 14:55:56,817 [INFO] Evaluating: window=10, layers=2, units=48, dropout=0.344, lr=0.00641, batch=53
2025-07-24 14:56:01,960 [INFO] Evaluating: window=21, layers=2, units=52, dropout=0.417, lr=0.00991, batch=25
2025-07-24 14:56:07,293 [INFO] Evaluating: window=10, layers=2, units=48, dropout=0.353, lr=0.00647, batch=55
2025-07-24 14:56:12,347 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.179, lr=0.00990, batch=25
2025-07-24 14:56:18,206 [INFO] Evaluating: window=34, layers=2, units=35, dropout=0.431, lr=0.00385, batch=33
2025-07-24 14:56:23,618 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.413, lr=0.00979, batch=25
2025-07-24 14:56:28,961 [INFO] Evaluating: window=22, layers=2, units=48, dropout=0.423, lr=0.00639, batch=26
2025-07-24 14:56:34,453 [INFO] Evaluating: window=20, layers=2, units=49, dropout=0.167, lr=0.00588, batch=25
2025-07-24 14:56:40,063 [INFO] Evaluating: window=34, layers=2, units=37, dropout=0.440, lr=0.00391, batch=29
2025-07-24 14:56:45,753 [INFO] Evaluating: window=21, layers=2, units=48, dropout=0.164, lr=0.00990, batch=25
2025-07-24 14:56:51,198 [INFO] Evaluating: window=10, layers=2, units=48, dropout=0.353, lr=0.00648, batch=55
2025-07-24 14:56:58,268 [INFO] Evaluating: window=10, layers=2, units=48, dropout=0.362, lr=0.00648, batch=55
2025-07-24 14:57:04,823 [INFO] Evaluating: window=21, layers=2, units=48, dropout=0.164, lr=0.00990, batch=25
2025-07-24 14:57:10,801 [INFO] Evaluating: window=34, layers=2, units=52, dropout=0.309, lr=0.00446, batch=26
2025-07-24 14:57:16,989 [INFO] Evaluating: window=34, layers=2, units=36, dropout=0.361, lr=0.00876, batch=55
2025-07-24 14:57:22,968 [INFO] Evaluating: window=19, layers=2, units=48, dropout=0.164, lr=0.00991, batch=25
2025-07-24 14:57:29,351 [INFO] Evaluating: window=34, layers=2, units=49, dropout=0.309, lr=0.00530, batch=54
2025-07-24 14:57:35,408 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.173, lr=0.00991, batch=25
2025-07-24 14:57:41,904 [INFO] Evaluating: window=34, layers=2, units=36, dropout=0.431, lr=0.00870, batch=26
2025-07-24 14:57:48,428 [INFO] Evaluating: window=11, layers=2, units=48, dropout=0.353, lr=0.00666, batch=55
2025-07-24 14:57:54,638 [INFO] Evaluating: window=10, layers=2, units=48, dropout=0.353, lr=0.00677, batch=54
2025-07-24 14:58:00,381 [INFO] Evaluating: window=11, layers=2, units=36, dropout=0.431, lr=0.00386, batch=27
2025-07-24 14:58:06,367 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.417, lr=0.00996, batch=25
2025-07-24 14:58:12,406 [INFO] Evaluating: window=10, layers=2, units=58, dropout=0.353, lr=0.00645, batch=28
2025-07-24 14:58:18,727 [INFO] Evaluating: window=11, layers=2, units=48, dropout=0.164, lr=0.00973, batch=56
2025-07-24 14:58:24,861 [INFO] Evaluating: window=22, layers=2, units=36, dropout=0.400, lr=0.00424, batch=27
2025-07-24 14:58:31,854 [INFO] Evaluating: window=10, layers=2, units=49, dropout=0.367, lr=0.00647, batch=55
2025-07-24 14:58:38,490 [INFO] Evaluating: window=34, layers=2, units=49, dropout=0.310, lr=0.00469, batch=27
2025-07-24 14:58:45,879 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.417, lr=0.00991, batch=23
2025-07-24 14:58:52,503 [INFO] Evaluating: window=38, layers=2, units=48, dropout=0.431, lr=0.00641, batch=26
2025-07-24 14:58:59,307 [INFO] Evaluating: window=10, layers=2, units=36, dropout=0.361, lr=0.00878, batch=58
2025-07-24 14:59:05,687 [INFO] Evaluating: window=35, layers=2, units=52, dropout=0.431, lr=0.00385, batch=27
2025-07-24 14:59:12,201 [INFO] Evaluating: window=35, layers=2, units=42, dropout=0.164, lr=0.00944, batch=57
2025-07-24 14:59:18,809 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.420, lr=0.00990, batch=25
2025-07-24 14:59:25,183 [INFO] Evaluating: window=10, layers=2, units=49, dropout=0.353, lr=0.00647, batch=52
2025-07-24 14:59:31,578 [INFO] Evaluating: window=11, layers=2, units=49, dropout=0.352, lr=0.00551, batch=55
2025-07-24 14:59:37,847 [INFO] Evaluating: window=20, layers=2, units=49, dropout=0.164, lr=0.00990, batch=25
2025-07-24 14:59:44,632 [INFO] Evaluating: window=10, layers=2, units=37, dropout=0.353, lr=0.00581, batch=55
2025-07-24 14:59:51,303 [INFO] Evaluating: window=22, layers=2, units=32, dropout=0.431, lr=0.00871, batch=27
2025-07-24 14:59:58,343 [INFO] Evaluating: window=34, layers=2, units=36, dropout=0.312, lr=0.00469, batch=27
2025-07-24 15:00:05,175 [INFO] Evaluating: window=11, layers=1, units=48, dropout=0.161, lr=0.00285, batch=16
2025-07-24 15:00:10,533 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.162, lr=0.00991, batch=25
2025-07-24 15:00:17,386 [INFO] Evaluating: window=20, layers=2, units=36, dropout=0.362, lr=0.00990, batch=25
2025-07-24 15:00:24,284 [INFO] Evaluating: window=35, layers=2, units=48, dropout=0.308, lr=0.00653, batch=55
2025-07-24 15:00:31,772 [INFO] Evaluating: window=34, layers=2, units=48, dropout=0.164, lr=0.00979, batch=56
2025-07-24 15:00:38,956 [INFO] Evaluating: window=21, layers=2, units=48, dropout=0.164, lr=0.00990, batch=29
2025-07-24 15:00:46,643 [INFO] Evaluating: window=34, layers=2, units=36, dropout=0.424, lr=0.00385, batch=27
2025-07-24 15:00:53,657 [INFO] Evaluating: window=19, layers=2, units=48, dropout=0.164, lr=0.00973, batch=56
2025-07-24 15:01:00,476 [INFO] Evaluating: window=10, layers=2, units=48, dropout=0.351, lr=0.00881, batch=25
2025-07-24 15:01:07,749 [INFO] Evaluating: window=10, layers=2, units=49, dropout=0.419, lr=0.00878, batch=53
2025-07-24 15:01:14,533 [INFO] Evaluating: window=10, layers=2, units=52, dropout=0.353, lr=0.00663, batch=27
2025-07-24 15:01:22,428 [INFO] Evaluating: window=11, layers=2, units=36, dropout=0.431, lr=0.00373, batch=27
2025-07-24 15:01:29,761 [INFO] Evaluating: window=19, layers=1, units=48, dropout=0.164, lr=0.00990, batch=25
2025-07-24 15:01:36,336 [INFO] Evaluating: window=38, layers=1, units=41, dropout=0.190, lr=0.00259, batch=17
2025-07-24 15:01:43,215 [INFO] Evaluating: window=11, layers=2, units=48, dropout=0.417, lr=0.00991, batch=25
2025-07-24 15:01:50,634 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.419, lr=0.00655, batch=55
2025-07-24 15:01:57,748 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.164, lr=0.00990, batch=24
2025-07-24 15:02:05,368 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.420, lr=0.00990, batch=27
2025-07-24 15:02:12,991 [INFO] Evaluating: window=34, layers=2, units=52, dropout=0.409, lr=0.00469, batch=27
2025-07-24 15:02:20,737 [INFO] Evaluating: window=10, layers=2, units=49, dropout=0.442, lr=0.00955, batch=53
2025-07-24 15:02:28,059 [INFO] Evaluating: window=21, layers=2, units=48, dropout=0.163, lr=0.00990, batch=25
2025-07-24 15:02:34,987 [INFO] Evaluating: window=34, layers=2, units=48, dropout=0.309, lr=0.00421, batch=27
2025-07-24 15:02:42,419 [INFO] Evaluating: window=10, layers=2, units=49, dropout=0.392, lr=0.00878, batch=53
2025-07-24 15:02:49,502 [INFO] Evaluating: window=34, layers=2, units=36, dropout=0.431, lr=0.00385, batch=27
2025-07-24 15:02:57,422 [INFO] Evaluating: window=34, layers=2, units=36, dropout=0.431, lr=0.00385, batch=28
2025-07-24 15:03:05,475 [INFO] Evaluating: window=20, layers=2, units=49, dropout=0.301, lr=0.00991, batch=25
2025-07-24 15:03:13,368 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.140, lr=0.00990, batch=23
2025-07-24 15:03:21,422 [INFO] Evaluating: window=10, layers=2, units=49, dropout=0.354, lr=0.00655, batch=55
2025-07-24 15:03:29,731 [INFO] Evaluating: window=21, layers=2, units=52, dropout=0.164, lr=0.00990, batch=25
2025-07-24 15:03:38,510 [INFO] Evaluating: window=10, layers=2, units=48, dropout=0.351, lr=0.00647, batch=55
2025-07-24 15:03:46,226 [INFO] Evaluating: window=22, layers=2, units=48, dropout=0.164, lr=0.00910, batch=25
2025-07-24 15:03:54,510 [INFO] Evaluating: window=20, layers=2, units=48, dropout=0.417, lr=0.00996, batch=25
2025-07-24 15:04:02,399 [INFO] Fold 60: BO(n_calls=12)
2025-07-24 15:04:02,406 [INFO] Evaluating: window=34, layers=1, units=57, dropout=0.339, lr=0.00078, batch=21
2025-07-24 15:04:09,615 [INFO] Evaluating: window=24, layers=2, units=37, dropout=0.360, lr=0.00013, batch=51
2025-07-24 15:04:17,650 [INFO] Evaluating: window=38, layers=1, units=64, dropout=0.347, lr=0.00167, batch=16
2025-07-24 15:04:25,287 [INFO] Evaluating: window=11, layers=2, units=45, dropout=0.119, lr=0.00886, batch=27
2025-07-24 15:04:33,479 [INFO] Evaluating: window=13, layers=2, units=44, dropout=0.493, lr=0.00086, batch=57
2025-07-24 15:04:42,518 [INFO] Evaluating: window=30, layers=2, units=32, dropout=0.477, lr=0.00134, batch=34
2025-07-24 15:04:50,683 [INFO] Evaluating: window=10, layers=1, units=40, dropout=0.373, lr=0.00166, batch=56
2025-07-24 15:04:58,314 [INFO] Evaluating: window=15, layers=2, units=38, dropout=0.402, lr=0.00071, batch=26
2025-07-24 15:05:07,237 [INFO] Evaluating: window=27, layers=1, units=59, dropout=0.280, lr=0.00062, batch=60
2025-07-24 15:05:14,418 [INFO] Evaluating: window=32, layers=2, units=50, dropout=0.308, lr=0.00836, batch=57
2025-07-24 15:05:23,019 [INFO] Evaluating: window=27, layers=1, units=55, dropout=0.299, lr=0.00047, batch=62
2025-07-24 15:05:30,528 [INFO] Evaluating: window=40, layers=1, units=43, dropout=0.371, lr=0.00102, batch=24
2025-07-24 15:05:37,782 [INFO] â†’ Fold 60 done in 2244.3s | RMSE=0.4856
2025-07-24 15:05:37,782 [INFO] Starting fold 72
2025-07-24 15:05:37,792 [INFO] Fold 72: Volatility=Low, GA(pop=10, gen=8), BO(calls=10), top_n=3
2025-07-24 15:05:37,792 [INFO] Evaluating: window=32, layers=2, units=36, dropout=0.128, lr=0.00894, batch=34
2025-07-24 15:05:46,030 [INFO] Evaluating: window=18, layers=2, units=58, dropout=0.445, lr=0.00323, batch=56
2025-07-24 15:05:54,143 [INFO] Evaluating: window=11, layers=1, units=51, dropout=0.125, lr=0.00331, batch=48
2025-07-24 15:06:01,071 [INFO] Evaluating: window=20, layers=1, units=41, dropout=0.283, lr=0.00481, batch=53
2025-07-24 15:06:07,809 [INFO] Evaluating: window=32, layers=1, units=59, dropout=0.146, lr=0.00263, batch=55
2025-07-24 15:06:15,289 [INFO] Evaluating: window=21, layers=2, units=46, dropout=0.417, lr=0.00408, batch=36
2025-07-24 15:06:23,495 [INFO] Evaluating: window=21, layers=2, units=49, dropout=0.478, lr=0.00258, batch=20
2025-07-24 15:06:32,001 [INFO] Evaluating: window=21, layers=2, units=59, dropout=0.238, lr=0.00657, batch=32
2025-07-24 15:06:41,062 [INFO] Evaluating: window=28, layers=2, units=39, dropout=0.262, lr=0.00993, batch=57
2025-07-24 15:06:49,364 [INFO] Evaluating: window=26, layers=2, units=41, dropout=0.307, lr=0.00850, batch=48
2025-07-24 15:06:57,399 [INFO] Evaluating: window=22, layers=2, units=59, dropout=0.470, lr=0.00669, batch=32
2025-07-24 15:07:05,581 [INFO] Evaluating: window=32, layers=2, units=36, dropout=0.132, lr=0.00894, batch=36
2025-07-24 15:07:13,896 [INFO] Evaluating: window=21, layers=2, units=38, dropout=0.409, lr=0.00408, batch=36
2025-07-24 15:07:22,084 [INFO] Evaluating: window=32, layers=2, units=36, dropout=0.127, lr=0.00771, batch=34
2025-07-24 15:07:30,131 [INFO] Evaluating: window=18, layers=2, units=58, dropout=0.224, lr=0.00311, batch=56
2025-07-24 15:07:38,288 [INFO] Evaluating: window=23, layers=2, units=46, dropout=0.413, lr=0.00408, batch=32
2025-07-24 15:07:46,845 [INFO] Evaluating: window=26, layers=2, units=46, dropout=0.315, lr=0.00850, batch=48
2025-07-24 15:07:55,374 [INFO] Evaluating: window=21, layers=2, units=59, dropout=0.240, lr=0.00657, batch=32
2025-07-24 15:08:03,810 [INFO] Evaluating: window=32, layers=2, units=37, dropout=0.494, lr=0.00183, batch=34
2025-07-24 15:08:12,240 [INFO] Evaluating: window=21, layers=2, units=49, dropout=0.124, lr=0.00964, batch=20
2025-07-24 15:08:20,820 [INFO] Evaluating: window=20, layers=2, units=46, dropout=0.124, lr=0.00434, batch=20
2025-07-24 15:08:29,353 [INFO] Evaluating: window=21, layers=2, units=38, dropout=0.434, lr=0.00408, batch=36
2025-07-24 15:08:37,945 [INFO] Evaluating: window=21, layers=2, units=59, dropout=0.238, lr=0.00657, batch=21
2025-07-24 15:08:46,700 [INFO] Evaluating: window=21, layers=2, units=59, dropout=0.250, lr=0.00657, batch=32
2025-07-24 15:08:55,267 [INFO] Evaluating: window=20, layers=2, units=49, dropout=0.423, lr=0.00481, batch=21
2025-07-24 15:09:03,691 [INFO] Evaluating: window=15, layers=2, units=49, dropout=0.417, lr=0.00937, batch=36
2025-07-24 15:09:12,080 [INFO] Evaluating: window=21, layers=2, units=49, dropout=0.478, lr=0.00258, batch=18
2025-07-24 15:09:20,598 [INFO] Evaluating: window=21, layers=2, units=49, dropout=0.491, lr=0.00258, batch=32
2025-07-24 15:09:29,242 [INFO] Evaluating: window=21, layers=2, units=59, dropout=0.238, lr=0.00657, batch=32
2025-07-24 15:09:37,857 [INFO] Evaluating: window=21, layers=2, units=42, dropout=0.111, lr=0.00917, batch=36
2025-07-24 15:09:46,836 [INFO] Evaluating: window=21, layers=2, units=41, dropout=0.238, lr=0.00912, batch=36
2025-07-24 15:09:55,403 [INFO] Evaluating: window=21, layers=2, units=39, dropout=0.406, lr=0.00408, batch=32
2025-07-24 15:10:04,071 [INFO] Evaluating: window=28, layers=2, units=58, dropout=0.262, lr=0.00985, batch=57
2025-07-24 15:10:12,884 [INFO] Evaluating: window=20, layers=2, units=59, dropout=0.240, lr=0.00657, batch=32
2025-07-24 15:10:21,662 [INFO] Evaluating: window=21, layers=2, units=59, dropout=0.111, lr=0.00662, batch=20
2025-07-24 15:10:31,269 [INFO] Evaluating: window=21, layers=2, units=59, dropout=0.238, lr=0.00657, batch=36
2025-07-24 15:10:41,058 [INFO] Evaluating: window=22, layers=2, units=39, dropout=0.238, lr=0.00690, batch=32
2025-07-24 15:10:51,186 [INFO] Evaluating: window=21, layers=2, units=42, dropout=0.117, lr=0.00917, batch=36
